+++
title = "Argo vs Airflow"
date =  2023-03-23T11:26:58+08:00
description= "argo vs airflow"
weight = 1
+++

- [什么是Airflow?](#什么是airflow)
- [什么是Argo?](#什么是argo)
- [Argo vs Airflow](#argo-vs-airflow)
  - [工作流语言](#工作流语言)
  - [任务调度](#任务调度)
  - [可扩展性](#可扩展性)
  - [第三方集成](#第三方集成)
  - [支持工作流](#支持工作流)
  - [与k8s资源集成](#与k8s资源集成)
- [总结](#总结)

自动化在各行各业提高生产率和工作效率方面发挥着关键作用。最近，市场上出现了大量新的自动化和管理工具，因此，用户很难从新时代的技术池中为他们的用例选择最好的工具。

## 什么是Airflow?

Airflow允许组织以标准Python编程语言的有向无环图(dag)形式编写工作流，确保任何对该语言有最低限度了解的人都可以部署一个。每个DAG包含节点和连接器，节点通过连接器连接到其他节点以生成依赖树。Airflow通过指定流动的计划和频率来帮助组织安排他们的任务。Airflow还提供了一个交互式界面，以及一堆不同的工具来实时监控工作流程。

Apache Airflow在处理大量数据收集、处理和分析的组织中非常受欢迎。每个IT专家都有不同的工作或工作流程要执行，从从其他来源收集数据到处理数据、上传数据和创建报告。每天都有许多任务需要专家手动执行。Airflow触发自动工作流，减少了从各种来源收集数据、处理数据、上传数据并最终创建报告所需的时间和精力。

特性：
- 开源
- 动态集成： Airflow使用Python编程语言将工作流编写为dag。这允许Airflow与多个操作符、挂钩和连接器集成，以生成动态管道。它还可以轻松地与其他平台集成，如亚马逊AWS，微软Azure，谷歌云等。
- 可定制性：Airflow支持自定义，它允许用户设计自己的自定义操作符、执行器和钩子。您还可以根据需要扩展库，使其符合所需的抽象级别。
- 丰富的用户接口
- 可扩展：Airflow具有高度的可伸缩性，旨在同时支持多个相互依赖的工作流。

## 什么是Argo?

Argo是一个开源的工作流引擎，用于在Kubernetes上编排任务。由Applatex引入的Argo允许您完全在Kubernetes集群上创建和运行高级工作流。Argo工作流构建在Kubernetes之上，每个任务都作为一个单独的Kubernetes pod运行。业内许多知名组织将Argo工作流用于ML(机器学习)、ETL(提取、转换、加载)、数据处理和CI/CD管道。

Argo基本上是Kubernetes的扩展，因此，它是使用Kubernetes安装的。Argo工作流允许组织使用YAML将其任务定义为dag。Argo提供了用于审计的原生工作流存档，用于计划工作流的Cron工作流，以及功能齐全的REST API。Argo提供了一系列杀手级功能，使其与同类产品区别开来，让我们来看看它们。

特性：
- 开源： 云原生cncf项目
- 本地集成:Argo提供了本地工件支持，可以在运行时下载、传输和上传文件。它支持任何S3兼容的工件库，如AWS, GCS，阿里云OSS, HTTP, Git, Raw和Minio。
- 可伸缩性:Argo工作流具有健壮的重试机制，具有高可靠性和高可伸缩性。它能够并行管理数千个pod和工作流。
- 可定制性:Argo是高度可定制的，它支持模板和可组合性来创建和重用工作流。
- 强大的用户界面:Argo提供了一个功能齐全的用户界面(UI)，易于使用。Argo工作流v3.0 UI也支持Argo事件，更加健壮可靠。它有可嵌入的小部件和一个新的工作流日志查看器。

## Argo vs Airflow

Airflow和Argo都允许您将工作流定义为dag，但这两个平台的操作方式存在一些差异，这对于选择适合您的需求的平台至关重要。

### 工作流语言

Argo Workflow和Airflow的第一个关键区别是用于定义dag的编程语言。正如前面几节所讨论的，workflow允许组织用标准的Python编程语言将工作流定义为dag。Airflow运行Python生态系统中的每个任务。对Python有基本的了解就足以编写代码并简化复杂的管道和工作流。它基于python的API是其广泛流行和适应性的主要原因之一。

Argo同样允许组织将其工作流定义为dag，但与Airflow不同的是，这些定义是用YAML而不是Python编写的。Argo以Kubernetes pod的方式运行每个任务。然而，工作流通常是复杂的，复杂的流程最好用代码而不是YAML这样的配置语言来表示。

### 任务调度

Airflow擅长在计划上运行任务，它有一个容错调度器，能够识别什么时候错过了计划。不幸的是，调度器不能在高可用性或繁忙的设置中运行，因为它是系统的单点故障。但是，Airflow调度程序最多需要5分钟来重新扫描DAG文件进行更新，并执行状态循环来调度新任务。因此，它不支持低延迟调度。

Argo也很擅长运行预定的任务，但是如果控制器在预定的时间间隔内面临中断，它只能重新安排1个错过的任务。它将重新安排一个错过的任务，直到startingdeadlinesecseconds间隔设置。但是，如果中断持续时间超过startingdeadlinesecseconds，则不会重新调度任何任务。然而，Argo调度器接收来自Kubernetes的事件，并能够立即响应新的工作流和状态变化，而不需要状态循环，使其成为低延迟调度的理想选择。


### 可扩展性

Airflow支持水平可伸缩性，并能够同时运行多个调度程序。说到任务，“Airflow”依赖于一个专门的工作人员池来执行任务。因此，最大的任务并行度等于活动工人的数量。

Argo将每个任务作为一个单独的Kubernetes pod运行，因此它能够并行管理数千个pod和工作流。与《Airflow》不同的是，在《Argo》中，工作流的并行性不受固定工人数量的限制。因此，它最适合具有顺序和并行步骤依赖性的作业。

### 第三方集成

Airflow使用Python编程语言将工作流编写为dag。这允许Airflow连接到几乎任何第三方系统。Airflow也有自己的社区支持的数据库、云服务、计算集群等操作库。

Argo是一个开源容器，没有预先打包的操作符来连接第三方系统。但是，它支持任何S3兼容的Artifact Repository，如AWS、GCS、阿里云OSS、HTTP等，在运行时下载、传输和上传您的文件。

### 支持工作流

Airflow dag是静态的，一旦定义，它们就不能在运行时添加或修改步骤。Airflow仅按照计划运行dag，因此外部系统不能触发工作流运行。这意味着2个DAG运行不能同时启动。最重要的是，《Airflow》假设所有DAG都是独立的，因此它没有一级机制来将参数传递给DAG运行。

在Argo中，可以为工作流的每次运行动态地创建DAG定义。它可以将动态生成的结果列表上的任务映射为并行处理项。Argo workflow v3.0还支持Argo Events，这是一个致力于事件驱动工作流自动化的农业生态系统项目。Agro的参数传递语法允许您在任务级别传递输入和输出参数，并在工作流级别传递输入参数。

### 与k8s资源集成

Airflow有一个Kubernetes操作符，可以用来运行吊舱作为工作流程的一部分。但是，它不支持创建其他资源。

Argo构建在Kubernetes之上，每个任务都作为一个单独的Kubernetes pod运行。Argo有一个特殊的支持系统，可以在Kubernetes对象(如pod和部署)上执行CRUD操作。

## 总结

| 特性  | Argo  |Airflow|
|---|---|--|
| 工作流语言  | YAML  | Python |
|  低延迟调度  | Yes  | No |
| 高并行  | Yes  | No |
|  第三方集成 | No  | Yes |
|  动态工作流 | Yes  | No |
| 事件驱动工作流  | Yes  | No |
| 参数化工作流  | Yes  | No |
|  K8s集成 | Yes  | No |


没有银弹来决定哪种工具是最好的。选择在很大程度上取决于您的用例、需求和运行环境。

Argo和Airflow都允许您将任务定义为dag，但airflow更通用，而Argo在与第三方服务交互方面提供有限的灵活性。如果您已经在大部分基础设施中使用Kubernetes，建议您使用Argo完成任务。如果你的开发人员更习惯于用Python而不是YAML来编写DAG定义，你可以考虑使用Airflow。
